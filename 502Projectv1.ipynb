{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-73-89.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=project>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"project\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.4\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-73-89.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f15cc6d9dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('context') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"article_type\", StringType(), True),\n",
    "    StructField(\"np1\", StringType(), True),\n",
    "    StructField(\"np2\", StringType(), True),\n",
    "    StructField(\"context\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"time\", StringType(), True),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3://anly502project/data/part-r-00000\",sep = \"\\t\",header=False,schema=schema)\n",
    "df_2 = spark.read.csv(\"s3://anly502project/data/part-r-00001\",sep = \"\\t\",header=False,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_type: string (nullable = true)\n",
      " |-- np1: string (nullable = true)\n",
      " |-- np2: string (nullable = true)\n",
      " |-- context: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- article_type: string (nullable = true)\n",
      " |-- np1: string (nullable = true)\n",
      " |-- np2: string (nullable = true)\n",
      " |-- context: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Data Schema\n",
    "df.printSchema()\n",
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df = unionAll([df, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---+--------------------+-------------+--------------------+--------------------+-----+\n",
      "|article_type|            np1|np2|             context|       source|            category|            location| time|\n",
      "+------------+---------------+---+--------------------+-------------+--------------------+--------------------+-----+\n",
      "|     article|    Dark Knight|  E|  arg1 and Wall arg2|             |intlnews topstor ...|      , kerala india|14299|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |topstor,health,sc...|                   ,|14660|\n",
      "|     article|    Communities|  E|arg1 mobilised in...|             |    politics topstor|                   ,|14026|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |topstor,health,sc...|                   ,|14660|\n",
      "|            |     Coast bias|  E|arg2 is for East ...|             |      sports topstor| columbus, ohio u...|13956|\n",
      "|     article|Commerce office|  E|arg1 at DDD Linco...|             |topstor,lifestle,...| canton, ohio uni...|14363|\n",
      "|     article| 75-minute mark|  E|arg1 in a pulsati...|Yahoo! Sports|      topstor,sports|                   ,|14779|\n",
      "|     article| Brigham Circle|  E|      arg2 past arg1|             |             topstor| boston, massachu...|14434|\n",
      "|            | Brigham Circle|  E| arg2 trains at arg1|             |localnews headlin...| boston, massachu...|13862|\n",
      "|     article| Brigham Circle|  E|arg2 line service...|             |             topstor| boston, massachu...|14344|\n",
      "+------------+---------------+---+--------------------+-------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### show combined\n",
    "unioned_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "split_col = split(unioned_df['category'], ',')\n",
    "unioned_df = unioned_df.withColumn('category', split_col.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col_2 = split(unioned_df['category'], ' ')\n",
    "unioned_df = unioned_df.withColumn('category', split_col_2.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col_3 = split(unioned_df['category'], '_')\n",
    "unioned_df = unioned_df.withColumn('category', split_col_3.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col_4 = split(unioned_df['category'], '-')\n",
    "unioned_df = unioned_df.withColumn('category', split_col_4.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---+--------------------+-------------+---------+--------------------+-----+\n",
      "|article_type|            np1|np2|             context|       source| category|            location| time|\n",
      "+------------+---------------+---+--------------------+-------------+---------+--------------------+-----+\n",
      "|     article|    Dark Knight|  E|  arg1 and Wall arg2|             | intlnews|      , kerala india|14299|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |  topstor|                   ,|14660|\n",
      "|     article|    Communities|  E|arg1 mobilised in...|             | politics|                   ,|14026|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |  topstor|                   ,|14660|\n",
      "|            |     Coast bias|  E|arg2 is for East ...|             |   sports| columbus, ohio u...|13956|\n",
      "|     article|Commerce office|  E|arg1 at DDD Linco...|             |  topstor| canton, ohio uni...|14363|\n",
      "|     article| 75-minute mark|  E|arg1 in a pulsati...|Yahoo! Sports|  topstor|                   ,|14779|\n",
      "|     article| Brigham Circle|  E|      arg2 past arg1|             |  topstor| boston, massachu...|14434|\n",
      "|            | Brigham Circle|  E| arg2 trains at arg1|             |localnews| boston, massachu...|13862|\n",
      "|     article| Brigham Circle|  E|arg2 line service...|             |  topstor| boston, massachu...|14344|\n",
      "+------------+---------------+---+--------------------+-------------+---------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Show data frame after filtering the category\n",
    "unioned_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pip = pipeline.fit(unioned_df).transform(unioned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---+--------------------+-------------+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|article_type|            np1|np2|             context|       source|     category|            location| time|            document|               token|          normalized|               lemma|         clean_lemma|finished_clean_lemma|\n",
      "+------------+---------------+---+--------------------+-------------+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     article|    Dark Knight|  E|  arg1 and Wall arg2|             |     intlnews|      , kerala india|14299|[[document, 0, 17...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|    [arg, wall, arg]|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |      topstor|                   ,|14660|[[document, 0, 37...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, carotenoid,...|\n",
      "|     article|    Communities|  E|arg1 mobilised in...|             |     politics|                   ,|14026|[[document, 0, 34...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, mobilise, a...|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |      topstor|                   ,|14660|[[document, 0, 37...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, carotenoid,...|\n",
      "|            |     Coast bias|  E|arg2 is for East ...|             |       sports| columbus, ohio u...|13956|[[document, 0, 20...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|    [arg, east, arg]|\n",
      "|     article|Commerce office|  E|arg1 at DDD Linco...|             |      topstor| canton, ohio uni...|14363|[[document, 0, 27...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, ddd, lincol...|\n",
      "|     article| 75-minute mark|  E|arg1 in a pulsati...|Yahoo! Sports|      topstor|                   ,|14779|[[document, 0, 29...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, pulsate, gr...|\n",
      "|     article| Brigham Circle|  E|      arg2 past arg1|             |      topstor| boston, massachu...|14434|[[document, 0, 13...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|    [arg, past, arg]|\n",
      "|            | Brigham Circle|  E| arg2 trains at arg1|             |    localnews| boston, massachu...|13862|[[document, 0, 18...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|   [arg, train, arg]|\n",
      "|     article| Brigham Circle|  E|arg2 line service...|             |      topstor| boston, massachu...|14344|[[document, 0, 26...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, line, servi...|\n",
      "|     article| Drill Sergeant|  E|arg1 Mindstrong i...|             |      topstor|    ,  united states|14417|[[document, 0, 28...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, mindstrong,...|\n",
      "|     article| Brigham Circle|  E|arg2 line will st...|             |      topstor| boston, massachu...|14344|[[document, 0, 36...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, line, stop,...|\n",
      "|     article| Brigham Circle|  E|arg2 line will st...|             |      topstor| boston, massachu...|14344|[[document, 0, 36...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, line, stop,...|\n",
      "|     article| Brigham Circle|  E|arg2 line beyond ...|             |      topstor| boston, massachu...|14344|[[document, 0, 20...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, line, beyon...|\n",
      "|     article| Brigham Circle|  E|arg2 line beyond ...|             |      topstor| boston, massachu...|14344|[[document, 0, 20...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, line, beyon...|\n",
      "|     article|           Cold|  E|arg2 Cape cold sn...|  iafrica.com| nationalnews|cape town, johann...|13991|[[document, 0, 29...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, cape, cold,...|\n",
      "|            |           Cook|  E|arg1 Off is rated...|             |     business|                    |13598|[[document, 0, 21...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|    [arg, rate, arg]|\n",
      "|            |           Boob|  E|    arg1 How my arg2|             |     business|        dayton, ohio|13763|[[document, 0, 15...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|          [arg, arg]|\n",
      "|            |           2002|  E|arg2 Street Band ...|             |entertainment| akron, ohio unit...|13793|[[document, 0, 26...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[arg, street, ban...|\n",
      "|            |           Boob|  E|    arg1 How my arg2|             |     business|        dayton, ohio|13762|[[document, 0, 15...|[[token, 0, 3, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|[[token, 0, 2, ar...|          [arg, arg]|\n",
      "+------------+---------------+---+--------------------+-------------+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pip.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "context_words = df_pip.withColumn(\"exploded_text\", explode(col(\"finished_clean_lemma\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article_type',\n",
       " 'np1',\n",
       " 'np2',\n",
       " 'context',\n",
       " 'source',\n",
       " 'category',\n",
       " 'location',\n",
       " 'time',\n",
       " 'document',\n",
       " 'token',\n",
       " 'normalized',\n",
       " 'lemma',\n",
       " 'clean_lemma',\n",
       " 'finished_clean_lemma',\n",
       " 'exploded_text']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_words.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = context_words.groupby('finished_clean_lemma').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o635.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 23, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat com.johnsnowlabs.nlp.DocumentAssembler.assemble(DocumentAssembler.scala:98)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:124)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:123)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:335)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat com.johnsnowlabs.nlp.DocumentAssembler.assemble(DocumentAssembler.scala:98)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:124)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:123)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dabfedcc7e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o635.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 23, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat com.johnsnowlabs.nlp.DocumentAssembler.assemble(DocumentAssembler.scala:98)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:124)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:123)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:335)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAssembleNoExtras$1: (string) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat com.johnsnowlabs.nlp.DocumentAssembler.assemble(DocumentAssembler.scala:98)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:124)\n\tat com.johnsnowlabs.nlp.DocumentAssembler$$anonfun$dfAssembleNoExtras$1.apply(DocumentAssembler.scala:123)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "counts_pd = counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|finished_clean_lemma                 |\n",
      "+-------------------------------------+\n",
      "|[arg, wall, arg]                     |\n",
      "|[arg, carotenoid, plus, vitamin, arg]|\n",
      "|[arg, mobilise, action, arg]         |\n",
      "|[arg, carotenoid, plus, vitamin, arg]|\n",
      "|[arg, east, arg]                     |\n",
      "|[arg, ddd, lincoln, way, arg]        |\n",
      "|[arg, pulsate, group, arg]           |\n",
      "|[arg, past, arg]                     |\n",
      "|[arg, train, arg]                    |\n",
      "|[arg, line, service, arg]            |\n",
      "|[arg, mindstrong, rate, arg]         |\n",
      "|[arg, line, stop, run, past, arg]    |\n",
      "|[arg, line, stop, run, past, arg]    |\n",
      "|[arg, line, beyond, arg]             |\n",
      "|[arg, line, beyond, arg]             |\n",
      "|[arg, cape, cold, snap, kill, arg]   |\n",
      "|[arg, rate, arg]                     |\n",
      "|[arg, arg]                           |\n",
      "|[arg, street, band, since, arg]      |\n",
      "|[arg, arg]                           |\n",
      "+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pip.select('finished_clean_lemma').show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|np1            |\n",
      "+---------------+\n",
      "|Dark Knight    |\n",
      "|Carotenoids    |\n",
      "|Communities    |\n",
      "|Carotenoids    |\n",
      "|Coast bias     |\n",
      "|Commerce office|\n",
      "|75-minute mark |\n",
      "|Brigham Circle |\n",
      "|Brigham Circle |\n",
      "|Brigham Circle |\n",
      "|Drill Sergeant |\n",
      "|Brigham Circle |\n",
      "|Brigham Circle |\n",
      "|Brigham Circle |\n",
      "|Brigham Circle |\n",
      "|Cold           |\n",
      "|Cook           |\n",
      "|Boob           |\n",
      "|2002           |\n",
      "|Boob           |\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pip.select('np1').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['np1', 'finished_clean_lemma', 'category']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_new = df_pip.withColumn(\"np1\",col('np1')).select(cols)\n",
    "df_2_new = df_pip.withColumn(\"finished_clean_lemma\", col('finished_clean_lemma')).select(cols)\n",
    "result = df_1_new.union(df_2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------+\n",
      "|            np1|finished_clean_lemma|     category|\n",
      "+---------------+--------------------+-------------+\n",
      "|    Dark Knight|    [arg, wall, arg]|     intlnews|\n",
      "|    Carotenoids|[arg, carotenoid,...|      topstor|\n",
      "|    Communities|[arg, mobilise, a...|     politics|\n",
      "|    Carotenoids|[arg, carotenoid,...|      topstor|\n",
      "|     Coast bias|    [arg, east, arg]|       sports|\n",
      "|Commerce office|[arg, ddd, lincol...|      topstor|\n",
      "| 75-minute mark|[arg, pulsate, gr...|      topstor|\n",
      "| Brigham Circle|    [arg, past, arg]|      topstor|\n",
      "| Brigham Circle|   [arg, train, arg]|    localnews|\n",
      "| Brigham Circle|[arg, line, servi...|      topstor|\n",
      "| Drill Sergeant|[arg, mindstrong,...|      topstor|\n",
      "| Brigham Circle|[arg, line, stop,...|      topstor|\n",
      "| Brigham Circle|[arg, line, stop,...|      topstor|\n",
      "| Brigham Circle|[arg, line, beyon...|      topstor|\n",
      "| Brigham Circle|[arg, line, beyon...|      topstor|\n",
      "|           Cold|[arg, cape, cold,...| nationalnews|\n",
      "|           Cook|    [arg, rate, arg]|     business|\n",
      "|           Boob|          [arg, arg]|     business|\n",
      "|           2002|[arg, street, ban...|entertainment|\n",
      "|           Boob|          [arg, arg]|     business|\n",
      "+---------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = result.select(split(col(\"np1\"),\",\")).alias(\"np1_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = result.withColumn(\"finished_clean_lemma\",col('finished_clean_lemma')).select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------+-----------------+\n",
      "|            np1|finished_clean_lemma|     category|            np1_1|\n",
      "+---------------+--------------------+-------------+-----------------+\n",
      "|    Dark Knight|    [arg, wall, arg]|     intlnews|    [Dark Knight]|\n",
      "|    Carotenoids|[arg, carotenoid,...|      topstor|    [Carotenoids]|\n",
      "|    Communities|[arg, mobilise, a...|     politics|    [Communities]|\n",
      "|    Carotenoids|[arg, carotenoid,...|      topstor|    [Carotenoids]|\n",
      "|     Coast bias|    [arg, east, arg]|       sports|     [Coast bias]|\n",
      "|Commerce office|[arg, ddd, lincol...|      topstor|[Commerce office]|\n",
      "| 75-minute mark|[arg, pulsate, gr...|      topstor| [75-minute mark]|\n",
      "| Brigham Circle|    [arg, past, arg]|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|   [arg, train, arg]|    localnews| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, servi...|      topstor| [Brigham Circle]|\n",
      "| Drill Sergeant|[arg, mindstrong,...|      topstor| [Drill Sergeant]|\n",
      "| Brigham Circle|[arg, line, stop,...|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, stop,...|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, beyon...|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, beyon...|      topstor| [Brigham Circle]|\n",
      "|           Cold|[arg, cape, cold,...| nationalnews|           [Cold]|\n",
      "|           Cook|    [arg, rate, arg]|     business|           [Cook]|\n",
      "|           Boob|          [arg, arg]|     business|           [Boob]|\n",
      "|           2002|[arg, street, ban...|entertainment|           [2002]|\n",
      "|           Boob|          [arg, arg]|     business|           [Boob]|\n",
      "+---------------+--------------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = result.withColumn('np1_1',split(col(\"np1\"),\",\"))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------+-----------------+\n",
      "|            np1|finished_clean_lemma|     category|            np1_1|\n",
      "+---------------+--------------------+-------------+-----------------+\n",
      "|    Dark Knight|[arg, wall, Dark ...|     intlnews|    [Dark Knight]|\n",
      "|    Carotenoids|[arg, carotenoid,...|      topstor|    [Carotenoids]|\n",
      "|    Communities|[arg, mobilise, a...|     politics|    [Communities]|\n",
      "|    Carotenoids|[arg, carotenoid,...|      topstor|    [Carotenoids]|\n",
      "|     Coast bias|[arg, east, Coast...|       sports|     [Coast bias]|\n",
      "|Commerce office|[arg, ddd, lincol...|      topstor|[Commerce office]|\n",
      "| 75-minute mark|[arg, pulsate, gr...|      topstor| [75-minute mark]|\n",
      "| Brigham Circle|[arg, past, Brigh...|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, train, Brig...|    localnews| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, servi...|      topstor| [Brigham Circle]|\n",
      "| Drill Sergeant|[arg, mindstrong,...|      topstor| [Drill Sergeant]|\n",
      "| Brigham Circle|[arg, line, stop,...|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, stop,...|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, beyon...|      topstor| [Brigham Circle]|\n",
      "| Brigham Circle|[arg, line, beyon...|      topstor| [Brigham Circle]|\n",
      "|           Cold|[arg, cape, cold,...| nationalnews|           [Cold]|\n",
      "|           Cook|   [arg, rate, Cook]|     business|           [Cook]|\n",
      "|           Boob|         [arg, Boob]|     business|           [Boob]|\n",
      "|           2002|[arg, street, ban...|entertainment|           [2002]|\n",
      "|           Boob|         [arg, Boob]|     business|           [Boob]|\n",
      "+---------------+--------------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, array, array_union\n",
    "\n",
    "df2 = df_new.withColumn(\"finished_clean_lemma\", array_union(\"finished_clean_lemma\", col('np1_1')))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|finished_clean_lemma|\n",
      "+--------------------+\n",
      "|[arg, wall, Dark ...|\n",
      "|[arg, carotenoid,...|\n",
      "|[arg, mobilise, a...|\n",
      "|[arg, carotenoid,...|\n",
      "|[arg, east, Coast...|\n",
      "|[arg, ddd, lincol...|\n",
      "|[arg, pulsate, gr...|\n",
      "|[arg, past, Brigh...|\n",
      "|[arg, train, Brig...|\n",
      "|[arg, line, servi...|\n",
      "|[arg, mindstrong,...|\n",
      "|[arg, line, stop,...|\n",
      "|[arg, line, stop,...|\n",
      "|[arg, line, beyon...|\n",
      "|[arg, line, beyon...|\n",
      "|[arg, cape, cold,...|\n",
      "|   [arg, rate, Cook]|\n",
      "|         [arg, Boob]|\n",
      "|[arg, street, ban...|\n",
      "|         [arg, Boob]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('finished_clean_lemma').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate = df2.select('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = cate.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(BoW_dict):\n",
    "    tot_words = sum(BoW_dict.values())\n",
    "    freq_dict = {word: BoW_dict[word]/tot_words for word in BoW_dict.keys()}\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def inverse_document_frequency(list_of_dicts):\n",
    "    tot_docs = len(list_of_dicts)\n",
    "    words = set([w for w_dict in list_of_dicts for w in w_dict.keys()])\n",
    "    idf_dict = {word: log(float(tot_docs)/(1.0+ sum([1 for w_dict in list_of_dicts if word in w_dict.keys()]))) for word in words}\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(list_of_dicts):\n",
    "    words = set([w for w_dict in list_of_dicts for w in w_dict.keys()])\n",
    "    tf_idf_dicts = []\n",
    "    idfs = inverse_document_frequency(list_of_dicts)\n",
    "    for w_dict in list_of_dicts:\n",
    "        w_dict.update({word: 0 for word in words if word not in w_dict.keys()})\n",
    "        tf = term_frequency(w_dict)\n",
    "        tf_idf_dicts.append({word: tf[word]*idfs[word] for word in words})\n",
    "    return tf_idf_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_word_dicts = df2.select('finished_clean_lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_list = map(lambda x: tf_idf(x), list_of_word_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-c3991467399f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_idf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'category' is not defined"
     ]
    }
   ],
   "source": [
    "tf_idf_dict = {c: tf_dict for c, tf_dict in zip(category, tf_idf_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.4.5\n",
      "Apache Spark version:  2.4.4\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "spark = sparknlp.start()\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
    "    .setInputCols([\"sentence\",'token'])\\\n",
    "    .setOutputCol(\"bert\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setPoolingLayer(0) # default 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession._instantiatedContext = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
