{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **502 Project** </center>\n",
    "### <center> Team member: Chenxi Liu, Nuo Tian, Mary Yu, Yuan Liu </center>\n",
    "\n",
    "### Data Selection: \n",
    "#### Yahoo News Data Set (Part 1 and 2of 35) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 : Data Extracting and Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-76-199.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=project>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"project\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.4\")\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-76-199.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa3c6f89c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Build Schema Using the README Instruction of the Data Set \n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"article_type\", StringType(), True),\n",
    "    StructField(\"np1\", StringType(), True),\n",
    "    StructField(\"np2\", StringType(), True),\n",
    "    StructField(\"context\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"time\", StringType(), True),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Readin the Data Set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3://anly502project/data/part-r-00000\",sep = \"\\t\",header=False,schema=schema)\n",
    "df_2 = spark.read.csv(\"s3://anly502project/data/part-r-00001\",sep = \"\\t\",header=False,schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Verify the Schema \n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_type: string (nullable = true)\n",
      " |-- np1: string (nullable = true)\n",
      " |-- np2: string (nullable = true)\n",
      " |-- context: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- article_type: string (nullable = true)\n",
      " |-- np1: string (nullable = true)\n",
      " |-- np2: string (nullable = true)\n",
      " |-- context: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Data Schema\n",
    "df.printSchema()\n",
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Combine the Data Set\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df = unionAll([df, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---+--------------------+-------------+--------------------+--------------------+-----+\n",
      "|article_type|            np1|np2|             context|       source|            category|            location| time|\n",
      "+------------+---------------+---+--------------------+-------------+--------------------+--------------------+-----+\n",
      "|     article|    Dark Knight|  E|  arg1 and Wall arg2|             |intlnews topstor ...|      , kerala india|14299|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |topstor,health,sc...|                   ,|14660|\n",
      "|     article|    Communities|  E|arg1 mobilised in...|             |    politics topstor|                   ,|14026|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |topstor,health,sc...|                   ,|14660|\n",
      "|            |     Coast bias|  E|arg2 is for East ...|             |      sports topstor| columbus, ohio u...|13956|\n",
      "|     article|Commerce office|  E|arg1 at DDD Linco...|             |topstor,lifestle,...| canton, ohio uni...|14363|\n",
      "|     article| 75-minute mark|  E|arg1 in a pulsati...|Yahoo! Sports|      topstor,sports|                   ,|14779|\n",
      "|     article| Brigham Circle|  E|      arg2 past arg1|             |             topstor| boston, massachu...|14434|\n",
      "|            | Brigham Circle|  E| arg2 trains at arg1|             |localnews headlin...| boston, massachu...|13862|\n",
      "|     article| Brigham Circle|  E|arg2 line service...|             |             topstor| boston, massachu...|14344|\n",
      "+------------+---------------+---+--------------------+-------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### show combined\n",
    "unioned_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Select the very top category by using multiple split \n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "split_col = split(unioned_df['category'], ',')\n",
    "unioned_df = unioned_df.withColumn('category', split_col.getItem(0))\n",
    "\n",
    "split_col_2 = split(unioned_df['category'], ' ')\n",
    "unioned_df = unioned_df.withColumn('category', split_col_2.getItem(0))\n",
    "\n",
    "split_col_3 = split(unioned_df['category'], '_')\n",
    "unioned_df = unioned_df.withColumn('category', split_col_3.getItem(0))\n",
    "\n",
    "split_col_4 = split(unioned_df['category'], '-')\n",
    "unioned_df = unioned_df.withColumn('category', split_col_4.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---+--------------------+-------------+---------+--------------------+-----+\n",
      "|article_type|            np1|np2|             context|       source| category|            location| time|\n",
      "+------------+---------------+---+--------------------+-------------+---------+--------------------+-----+\n",
      "|     article|    Dark Knight|  E|  arg1 and Wall arg2|             | intlnews|      , kerala india|14299|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |  topstor|                   ,|14660|\n",
      "|     article|    Communities|  E|arg1 mobilised in...|             | politics|                   ,|14026|\n",
      "|     article|    Carotenoids|  E|arg1 and caroteno...|             |  topstor|                   ,|14660|\n",
      "|            |     Coast bias|  E|arg2 is for East ...|             |   sports| columbus, ohio u...|13956|\n",
      "|     article|Commerce office|  E|arg1 at DDD Linco...|             |  topstor| canton, ohio uni...|14363|\n",
      "|     article| 75-minute mark|  E|arg1 in a pulsati...|Yahoo! Sports|  topstor|                   ,|14779|\n",
      "|     article| Brigham Circle|  E|      arg2 past arg1|             |  topstor| boston, massachu...|14434|\n",
      "|            | Brigham Circle|  E| arg2 trains at arg1|             |localnews| boston, massachu...|13862|\n",
      "|     article| Brigham Circle|  E|arg2 line service...|             |  topstor| boston, massachu...|14344|\n",
      "+------------+---------------+---+--------------------+-------------+---------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Show data frame after filtering the category\n",
    "unioned_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            context1|\n",
      "+--------------------+\n",
      "|Dark Knight arg1 ...|\n",
      "|Carotenoids arg1 ...|\n",
      "|Communities arg1 ...|\n",
      "|Carotenoids arg1 ...|\n",
      "|Coast bias arg2 i...|\n",
      "|Commerce office a...|\n",
      "|75-minute mark ar...|\n",
      "|Brigham Circle ar...|\n",
      "|Brigham Circle ar...|\n",
      "|Brigham Circle ar...|\n",
      "|Drill Sergeant ar...|\n",
      "|Brigham Circle ar...|\n",
      "|Brigham Circle ar...|\n",
      "|Brigham Circle ar...|\n",
      "|Brigham Circle ar...|\n",
      "|Cold arg2 Cape co...|\n",
      "|Cook arg1 Off is ...|\n",
      "|Boob arg1 How my ...|\n",
      "|2002 arg2 Street ...|\n",
      "|Boob arg1 How my ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "unioned_df1 = unioned_df.select(concat(col(\"np1\"), lit(' '),col(\"context\")).alias('context1'))\n",
    "unioned_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+---+--------------------+--------------------+-------------+--------------------+-----+\n",
      "|            context1|article_type|                 np1|np2|             context|              source|     category|            location| time|\n",
      "+--------------------+------------+--------------------+---+--------------------+--------------------+-------------+--------------------+-----+\n",
      "|Dane arg1 gain In...|     article|                Dane|  E|arg1 gain In Grou...|                    |     intlnews|                   ,|14223|\n",
      "|Dubai arg2 arlier...|            |               Dubai|  E|arg2 arlier in th...|                    |    localnews| nampa, idaho uni...|13892|\n",
      "|Circle arg2 past ...|     article|              Circle|  E|arg2 past Brigham...|                    |      topstor| boston, massachu...|14434|\n",
      "|Champions League ...|     article|Champions League ...|  E|  arg1 in Group arg2|Yahoo! UK & Irela...| nationalnews|                   ,|14119|\n",
      "|100-acre parcel a...|     article|     100-acre parcel|  E|arg1 in planning ...|                    |      topstor| highlands ranch,...|14578|\n",
      "|100-acre parcel a...|     article|     100-acre parcel|  E|arg1 in planning ...|                    |      topstor| highlands ranch,...|14568|\n",
      "|BMW arg1 also unv...|     article|                 BMW|  E|arg1 also unveile...|                    |      topstor|                   ,|14595|\n",
      "|Chevrolet Astro m...|            |Chevrolet Astro m...|  E|arg1 headed west ...|                    |    localnews| racine, wisconsi...|13941|\n",
      "|Businesses arg1 w...|     article|          Businesses|  E|arg1 will pay les...|  Alameda Times-Star|      topstor| alameda, califor...|14763|\n",
      "|Born arg2 Street ...|     article|                Born|  E|arg2 Street Band ...|     Blogcritics.org|   mistopstor|                   ,|14543|\n",
      "|Bernardino arg2 S...|    cronkite|          Bernardino|  E|arg2 Street in Sa...|         Yahoo! News|       cirhdq|    ,  united states|14286|\n",
      "|Airport Thursday ...|     article|    Airport Thursday|  E|arg2 Concourse of...|                    |      topstor| milwaukee, wisco...|14693|\n",
      "|Canada arg1 repre...|     article|              Canada|  E|arg1 represents D...|New Orleans Times...|     politics| new orleans, lou...|14288|\n",
      "|2 arg2 and for Bl...|    cronkite|                   2|  E|arg2 and for Bloc...|         Yahoo! News|       cjezge|    ,  united states|14571|\n",
      "|11:45 p.m. arg2 s...|     article|          11:45 p.m.|  E|arg2 streets abou...|      Palladium-Item|      topstor| richmond, indian...|14393|\n",
      "|800 block arg2 Z ...|            |           800 block|  E|arg2 Z Go Car Sal...|        Sun-Sentinel|    localnews|fort lauderdale, ...|13639|\n",
      "|Avenue NW arg2 fr...|     article|           Avenue NW|  E|arg2 from Clevela...|                    |    localnews| canton, ohio uni...|14201|\n",
      "|2-1 win arg1 over...|     article|             2-1 win|  E|arg1 over Cameroo...|       Yahoo! Sports|      topstor|                   ,|14779|\n",
      "|Barrel arg1 Blast...|            |              Barrel|  E|arg1 Blast is rat...|                    |entertainment|    ,  united states|13794|\n",
      "|2010 World arg1 C...|       photo|          2010 World|  E| arg1 Cup group arg2|         Yahoo! News|      topstor|                   ,|14779|\n",
      "+--------------------+------------+--------------------+---+--------------------+--------------------+-------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df1 = unioned_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "df2 = unioned_df1.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "df3 = df2.join(df1, \"id\", \"outer\").drop(\"id\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Category (label) insight \n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Filter out the null and empty category \n",
    "df3 =df3.filter(df3.category.isNotNull())\n",
    "df3 = df3.filter(df3.category != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql as sql\n",
    "count_df = df3.groupBy(\"category\").count()\n",
    "count_df.createOrReplaceTempView(\"count_df\")\n",
    "count_rank_df = spark.sql(\"SELECT category, count FROM count_df ORDER BY count DESC LIMIT 15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Using the SQL to filter the category (label)\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|     category|   count|\n",
      "+-------------+--------+\n",
      "|      topstor|85027590|\n",
      "|    localnews|40174779|\n",
      "|       sports|22925153|\n",
      "|     business|20017107|\n",
      "| nationalnews| 5073191|\n",
      "|     intlnews| 4252106|\n",
      "|    technolog| 2546612|\n",
      "|entertainment| 2359067|\n",
      "|     politics| 2092784|\n",
      "|     lifestle| 1785076|\n",
      "+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_rank_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView(\"unioned_df\")\n",
    "count_rank_df.createOrReplaceTempView(\"count_rank_df\")\n",
    "df_final = spark.sql(\"SELECT * FROM unioned_df WHERE unioned_df.category IN (SELECT category FROM count_rank_df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+---+--------------------+--------------------+------------+--------------------+-----+\n",
      "|            context1|article_type|                 np1|np2|             context|              source|    category|            location| time|\n",
      "+--------------------+------------+--------------------+---+--------------------+--------------------+------------+--------------------+-----+\n",
      "|Dane arg1 gain In...|     article|                Dane|  E|arg1 gain In Grou...|                    |    intlnews|                   ,|14223|\n",
      "|Dubai arg2 arlier...|            |               Dubai|  E|arg2 arlier in th...|                    |   localnews| nampa, idaho uni...|13892|\n",
      "|Circle arg2 past ...|     article|              Circle|  E|arg2 past Brigham...|                    |     topstor| boston, massachu...|14434|\n",
      "|Champions League ...|     article|Champions League ...|  E|  arg1 in Group arg2|Yahoo! UK & Irela...|nationalnews|                   ,|14119|\n",
      "|100-acre parcel a...|     article|     100-acre parcel|  E|arg1 in planning ...|                    |     topstor| highlands ranch,...|14578|\n",
      "|100-acre parcel a...|     article|     100-acre parcel|  E|arg1 in planning ...|                    |     topstor| highlands ranch,...|14568|\n",
      "|BMW arg1 also unv...|     article|                 BMW|  E|arg1 also unveile...|                    |     topstor|                   ,|14595|\n",
      "|Chevrolet Astro m...|            |Chevrolet Astro m...|  E|arg1 headed west ...|                    |   localnews| racine, wisconsi...|13941|\n",
      "|Businesses arg1 w...|     article|          Businesses|  E|arg1 will pay les...|  Alameda Times-Star|     topstor| alameda, califor...|14763|\n",
      "|Airport Thursday ...|     article|    Airport Thursday|  E|arg2 Concourse of...|                    |     topstor| milwaukee, wisco...|14693|\n",
      "+--------------------+------------+--------------------+---+--------------------+--------------------+------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Filter the dataset by categories \n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    " 'politics',\n",
    "'science',\n",
    "'health',\n",
    "'technolog',\n",
    "'entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.filter(col('category').isin(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+---+--------------------+--------------------+-------------+--------------------+-----+\n",
      "|            context1|article_type|                 np1|np2|             context|              source|     category|            location| time|\n",
      "+--------------------+------------+--------------------+---+--------------------+--------------------+-------------+--------------------+-----+\n",
      "|Canada arg1 repre...|     article|              Canada|  E|arg1 represents D...|New Orleans Times...|     politics| new orleans, lou...|14288|\n",
      "|Barrel arg1 Blast...|            |              Barrel|  E|arg1 Blast is rat...|                    |entertainment|    ,  united states|13794|\n",
      "| 0157 arg2 coli arg1|            |                0157|  E|      arg2 coli arg1|                    |       health|                   ,|13840|\n",
      "|ASH arg1 was rate...|     article|                 ASH|  E| arg1 was rated arg2|                    |entertainment|    ,  united states|14021|\n",
      "|Comedy arg1 Explo...|            |              Comedy|  E|arg1 Explosion Sh...|                    |entertainment|                   ,|13840|\n",
      "|30 arg1 times bet...|     article|                  30|  E|arg1 times better...|                    |       health|                   ,|14029|\n",
      "|Coroner arg2 Wint...|            |             Coroner| Ed|arg2 Winter of th...|Long Beach Press-...|entertainment|long beach, calif...|13701|\n",
      "|Alan Black arg2 H...|     article|          Alan Black| Ed| arg2 Hefti and arg1|                    |entertainment| wichita falls, t...|13973|\n",
      "|Beach arg2 Saxon ...|     article|               Beach| Ed|arg2 Saxon and Bi...|                    |entertainment|                   ,|13976|\n",
      "|Chief Operating O...|     article|Chief Operating O...| Ed|arg2 Peterson wil...|                    |    technolog| state college, p...|14032|\n",
      "+--------------------+------------+--------------------+---+--------------------+--------------------+-------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.createOrReplaceTempView(\"df4\")\n",
    "data = spark.sql('select category,concat(np1,\"\\n\",context1) as text from df4')\n",
    "print(type(data))\n",
    "data.printSchema()\n",
    "#data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_label_fit = StringIndexer(inputCol ='category', outputCol =\"label\").fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+\n",
      "|     category|                text|label|\n",
      "+-------------+--------------------+-----+\n",
      "|     politics|Canada\n",
      "Canada arg...|  2.0|\n",
      "|entertainment|Barrel\n",
      "Barrel arg...|  1.0|\n",
      "|       health|0157\n",
      "0157 arg2 co...|  3.0|\n",
      "|entertainment|ASH\n",
      "ASH arg1 was ...|  1.0|\n",
      "|entertainment|Comedy\n",
      "Comedy arg...|  1.0|\n",
      "|       health|30\n",
      "30 arg1 times ...|  3.0|\n",
      "|entertainment|Coroner\n",
      "Coroner a...|  1.0|\n",
      "|entertainment|Alan Black\n",
      "Alan B...|  1.0|\n",
      "|entertainment|Beach\n",
      "Beach arg2 ...|  1.0|\n",
      "|    technolog|Chief Operating O...|  0.0|\n",
      "|     politics|Conference\n",
      "Confer...|  2.0|\n",
      "|     politics|Cabinet Office\n",
      "Ca...|  2.0|\n",
      "|entertainment|Blender\n",
      "Blender a...|  1.0|\n",
      "|     politics|Commons\n",
      "Commons a...|  2.0|\n",
      "|entertainment|Beverly Hills hom...|  1.0|\n",
      "|entertainment|Christmas gift\n",
      "Ch...|  1.0|\n",
      "|entertainment|Brandeis\n",
      "Brandeis...|  1.0|\n",
      "|entertainment|Chemical Brothers...|  1.0|\n",
      "|entertainment|Hunter\n",
      "Hunter arg...|  1.0|\n",
      "|entertainment|News\n",
      "News arg2 to...|  1.0|\n",
      "+-------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "si_label_fit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Build the pipeline\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelConverter = IndexToString(inputCol='category', outputCol='label', labels = si_label_fit.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setUseAbbreviations(True)\n",
    "    \n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"sentence\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "    \n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCols([\"ntokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, stemmer, normalizer, finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = nlp_pipeline.fit(si_label_fit).transform(si_label_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = processed.randomSplit(weights=[0.7, 0.3], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|     category|                text|label|             ntokens|        clean_tokens|                  tf|                 idf|\n",
      "+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|entertainment|%\n",
      "% arg1 of Oscar...|  1.0|[arg, of, oscar, ...|   [arg, oscar, arg]|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|*Results\n",
      "*Results...|  1.0|[result, result, ...|[result, result, ...|(500,[0,183,336],...|(500,[0,183,336],...|\n",
      "|entertainment|---\n",
      "--- arg1 with...|  1.0|[arg, with, it, arg]|          [arg, arg]|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|.... Former Presi...|  1.0|[former, presid, ...|[former, presid, ...|(500,[0,5,26,107,...|(500,[0,5,26,107,...|\n",
      "|entertainment|.com\n",
      ".com arg2 on...|  1.0|[com, com, arg, o...|[com, com, arg, p...|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|.com\n",
      ".com arg2 to...|  1.0|[com, com, arg, t...|[com, com, arg, t...|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|.org\n",
      ".org arg2 mo...|  1.0|[org, org, arg, m...|[org, org, arg, m...|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|.org\n",
      ".org arg2 mo...|  1.0|[org, org, arg, m...|[org, org, arg, m...|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|007\n",
      "007 arg2 from...|  1.0|[arg, from, hi, i...|[arg, hi, incarn,...|(500,[0,5],[2.0,1...|(500,[0,5],[0.0,4...|\n",
      "|entertainment|1\n",
      "1 arg1 million ...|  1.0|[arg, million, in...|[arg, million, we...|(500,[0,25],[2.0,...|(500,[0,25],[0.0,...|\n",
      "|entertainment|1 p.m.\n",
      "1 p.m. arg...|  1.0|[pm, pm, arg, at,...|[pm, pm, arg, dis...|(500,[0,372],[2.0...|(500,[0,372],[0.0...|\n",
      "|entertainment|1 percent\n",
      "1 perce...|  1.0|[percent, percent...|[percent, percent...|(500,[0,16],[2.0,...|(500,[0,16],[0.0,...|\n",
      "|entertainment|1,000\n",
      "1,000 arg1 ...|  1.0|[arg, addit, heli...|[arg, addit, heli...|(500,[0,275],[2.0...|(500,[0,275],[0.0...|\n",
      "|entertainment|1,000\n",
      "1,000 arg2 ...|  1.0|[arg, intend, to,...|[arg, intend, rep...|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|1,000 people\n",
      "1,00...|  1.0|[peopl, peopl, ar...|[peopl, peopl, ar...|(500,[0,30],[2.0,...|(500,[0,30],[0.0,...|\n",
      "|entertainment|1,000 police offi...|  1.0|[polic, offic, po...|[polic, offic, po...|(500,[0,82,128,27...|(500,[0,82,128,27...|\n",
      "|entertainment|1,128,000 househo...|  1.0|[household, house...|[household, house...|(500,[0,468],[2.0...|(500,[0,468],[0.0...|\n",
      "|entertainment|1,128,000 househo...|  1.0|[household, house...|[household, house...|(500,[0,468],[2.0...|(500,[0,468],[0.0...|\n",
      "|entertainment|1,200 entries\n",
      "1,2...|  1.0|[entri, entri, ar...|[entri, entri, ar...|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "|entertainment|1,300\n",
      "1,300 arg1 ...|  1.0|[arg, austinit, w...|[arg, austinit, a...|     (500,[0],[2.0])|     (500,[0],[0.0])|\n",
      "+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import feature as spark_ft\n",
    "\n",
    "stopWords = spark_ft.StopWordsRemover.loadDefaultStopWords('english')\n",
    "sw_remover = spark_ft.StopWordsRemover(inputCol='ntokens', outputCol='clean_tokens', stopWords=stopWords)\n",
    "tf = spark_ft.CountVectorizer(vocabSize=500, inputCol='clean_tokens', outputCol='tf')\n",
    "idf = spark_ft.IDF(minDocFreq=5, inputCol='tf', outputCol='idf')\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[sw_remover, tf, idf])\n",
    "feature_model = feature_pipeline.fit(train)\n",
    "\n",
    "train_featurized = feature_model.transform(train).persist()\n",
    "#train_featurized.count()\n",
    "train_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- ntokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- clean_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- idf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_featurized.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Fit the model\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import classification as spark_cls\n",
    "\n",
    "rf = spark_cls.RandomForestClassifier(labelCol=\"label\", featuresCol=\"idf\", numTrees=100)\n",
    "\n",
    "model = rf.fit(train_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     category|                text|label|             ntokens|        clean_tokens|                  tf|                 idf|       rawPrediction|         probability|prediction|\n",
      "+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|entertainment|% guarantee\n",
      "% gua...|  1.0|[guarante, guaran...|[guarante, guaran...|     (500,[0],[2.0])|     (500,[0],[0.0])|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|(909\n",
      "(909 arg2 Ca...|  1.0|[arg, carolipio, ...|[arg, carolipio, ...|(500,[0,298],[2.0...|(500,[0,298],[0.0...|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|(916\n",
      "(916 arg2 Mo...|  1.0|[arg, moor, at, arg]|    [arg, moor, arg]|     (500,[0],[2.0])|     (500,[0],[0.0])|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|.com\n",
      ".com arg2 te...|  1.0|[com, com, arg, t...|[com, com, arg, t...|     (500,[0],[2.0])|     (500,[0],[0.0])|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|.net Cheap Africa...|  1.0|[net, cheap, afri...|[net, cheap, afri...|(500,[0,63,160,22...|(500,[0,63,160,22...|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|1\n",
      "1 arg2 Fest wil...|  1.0|[arg, fest, will,...|[arg, fest, kick,...|     (500,[0],[2.0])|     (500,[0],[0.0])|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|1 a.m.\n",
      "1 a.m. arg...|  1.0|[am, am, arg, aft...|[arg, two, marath...|(500,[0,66],[2.0,...|(500,[0,66],[0.0,...|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|1 last year\n",
      "1 las...|  1.0|[last, year, last...|[last, year, last...|(500,[0,18,28],[2...|(500,[0,18,28],[0...|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|1,128,000 househo...|  1.0|[household, house...|[household, house...|(500,[0,468],[2.0...|(500,[0,468],[0.0...|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "|entertainment|1,128,000 househo...|  1.0|[household, house...|[household, house...|(500,[0,468],[2.0...|(500,[0,468],[0.0...|[29.6923665101603...|[0.29692366510160...|       0.0|\n",
      "+-------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_featurized = feature_model.transform(test)\n",
    "preds = model.transform(test_featurized)\n",
    "preds.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = preds.select('text', 'label', 'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>% guarantee\\n% guarantee arg1 a specific depar...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(909\\n(909 arg2 Carolipio can be reached at arg1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(916\\n(916 arg2 Moore at arg1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.com\\n.com arg2 tells Usmagazine arg1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.net Cheap Africa phone card\\n.net Cheap Afric...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552316</th>\n",
       "      <td>users\\nusers arg1 who test the fixes on arg2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552317</th>\n",
       "      <td>users\\nusers arg2 which may seem confusing to ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552318</th>\n",
       "      <td>victim\\nvictim arg1 of domestic arg2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552319</th>\n",
       "      <td>viewers\\nviewers arg2 of options to our arg1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552320</th>\n",
       "      <td>volcanoes\\nvolcanoes arg1 often give off arg2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2552321 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  label  prediction\n",
       "0        % guarantee\\n% guarantee arg1 a specific depar...    1.0         0.0\n",
       "1         (909\\n(909 arg2 Carolipio can be reached at arg1    1.0         0.0\n",
       "2                            (916\\n(916 arg2 Moore at arg1    1.0         0.0\n",
       "3                    .com\\n.com arg2 tells Usmagazine arg1    1.0         0.0\n",
       "4        .net Cheap Africa phone card\\n.net Cheap Afric...    1.0         0.0\n",
       "...                                                    ...    ...         ...\n",
       "2552316       users\\nusers arg1 who test the fixes on arg2    0.0         0.0\n",
       "2552317  users\\nusers arg2 which may seem confusing to ...    0.0         0.0\n",
       "2552318               victim\\nvictim arg1 of domestic arg2    0.0         0.0\n",
       "2552319       viewers\\nviewers arg2 of options to our arg1    0.0         0.0\n",
       "2552320      volcanoes\\nvolcanoes arg1 often give off arg2    0.0         0.0\n",
       "\n",
       "[2552321 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.342433\n",
      "Test Error = 0.657567\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(preds)\n",
    "\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
